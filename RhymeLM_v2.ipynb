{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RhymeLM v2 ‚Äî Dual-Corpus Character Language Model\n",
    "\n",
    "**Architecture Philosophy:**\n",
    "- Learn *what words look like* from the English dictionary (vocabulary grounding)\n",
    "- Learn *how artists flow* from rap lyrics (style, structure, rhyme schemes)\n",
    "- Combine both to generate coherent 16-bar verses\n",
    "\n",
    "**Key Upgrades from v1:**\n",
    "- LSTM backbone (better sequence modeling than feedforward)\n",
    "- Dual corpus: English dictionary + lyrics interleaved\n",
    "- Scaled dimensions: 256 embed, 512 hidden, 2 layers\n",
    "- CMU Pronouncing Dictionary integration for rhyme-aware vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Device selection (MPS for Mac, CUDA for GPU, else CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load & Prepare the Dual Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary corpus: 286,522 words\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2a. Load English Dictionary (Vocabulary Grounding)\n",
    "# -----------------------------------------\n",
    "import nltk\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('cmudict', quiet=True)\n",
    "\n",
    "from nltk.corpus import words as nltk_words\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Get all English words\n",
    "all_english = set(w.lower() for w in nltk_words.words())\n",
    "\n",
    "# Get CMU dictionary (words with pronunciation = rhymeable)\n",
    "cmu_dict = cmudict.dict()\n",
    "rhymeable_words = set(cmu_dict.keys())\n",
    "\n",
    "# Prioritize rhymeable words, but include common English words too\n",
    "# Filter to reasonable length (2-15 chars) for efficiency\n",
    "vocab_words = [w for w in rhymeable_words if 2 <= len(w) <= 15]\n",
    "vocab_words += [w for w in all_english if 2 <= len(w) <= 12 and w not in rhymeable_words]\n",
    "\n",
    "print(f\"Dictionary corpus: {len(vocab_words):,} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 530 tracks\n",
      "Columns: ['track_name', 'artist', 'raw_lyrics', 'artist_verses']\n",
      "Artists: 11 unique\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2b. Load Lyrics Corpus\n",
    "# -----------------------------------------\n",
    "CSV_PATH = \"lyrics_raw.csv\"  # Adjust path as needed\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df):,} tracks\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Artists: {df['artist'].nunique()} unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2,189 verse chunks (8-16 bars each)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2c. Extract 16-Bar Verses from Lyrics\n",
    "# -----------------------------------------\n",
    "def extract_verses(texts, min_bars=8, max_bars=16):\n",
    "    \"\"\"Split lyrics into chunks of 8-16 bars (lines).\"\"\"\n",
    "    verses = []\n",
    "    for txt in texts:\n",
    "        if not isinstance(txt, str):\n",
    "            continue\n",
    "        lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]\n",
    "        \n",
    "        # Clean out common noise\n",
    "        lines = [ln for ln in lines if not ln.startswith('See ') \n",
    "                 and 'tickets as low as' not in ln\n",
    "                 and 'You might also like' not in ln]\n",
    "        \n",
    "        for i in range(0, len(lines), max_bars):\n",
    "            chunk = lines[i:i + max_bars]\n",
    "            if len(chunk) >= min_bars:\n",
    "                verses.append(\"\\n\".join(chunk))\n",
    "    return verses\n",
    "\n",
    "lyrics_texts = df[\"artist_verses\"].dropna().tolist()\n",
    "sixteen_bar_verses = extract_verses(lyrics_texts)\n",
    "\n",
    "print(f\"Extracted {len(sixteen_bar_verses):,} verse chunks (8-16 bars each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SAMPLE VERSE:\n",
      "==================================================\n",
      "Young Money raised me, grew up out in Baisley\n",
      "Southside Jamaica, Queens, and it's crazy\n",
      "'Cause I'm still hood, Hollywood couldn't change me\n",
      "Shout out to my haters, sorry that you couldn't faze me\n",
      "Ain't being cocky, we just vindicated\n",
      "Best believe that when we done, this moment will be syndicated\n",
      "I don't know, this night just remind me of\n",
      "Everything they deprived me of (Yeah)\n",
      "Put your drinks up\n",
      "It's a celebration every time we link up\n",
      "We done did everything they can think of\n",
      "Greatness is what we on the brink of\n",
      "I wish that I could have this moment for life, for life, for life\n",
      "'Cause in this moment, I just feel so alive, alive, alive\n",
      "I wish that I could have this moment for life, for life, for life\n",
      "This is my moment, I just feel so alive, alive, alive\n"
     ]
    }
   ],
   "source": [
    "# Preview a sample verse\n",
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE VERSE:\")\n",
    "print(\"=\" * 50)\n",
    "print(random.choice(sixteen_bar_verses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 729 dictionary blocks\n",
      "\n",
      "Total corpus length: 1,614,351 characters\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2d. Build Combined Corpus\n",
    "# -----------------------------------------\n",
    "# Strategy: Interleave dictionary words with lyrics\n",
    "# This teaches the model both \"what words look like\" and \"how verses flow\"\n",
    "\n",
    "def build_dual_corpus(verses, dictionary_words, dict_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Build a training corpus that interleaves:\n",
    "    - Verse chunks (primary content)\n",
    "    - Dictionary word blocks (vocabulary grounding)\n",
    "    \n",
    "    dict_ratio: fraction of corpus that should be dictionary words\n",
    "    \"\"\"\n",
    "    corpus_parts = []\n",
    "    \n",
    "    # Shuffle dictionary words\n",
    "    shuffled_dict = dictionary_words.copy()\n",
    "    random.shuffle(shuffled_dict)\n",
    "    \n",
    "    # Calculate how many dictionary blocks we need\n",
    "    num_verses = len(verses)\n",
    "    dict_blocks_needed = int(num_verses * dict_ratio / (1 - dict_ratio))\n",
    "    \n",
    "    # Create dictionary blocks (groups of ~50 words separated by newlines)\n",
    "    words_per_block = 50\n",
    "    dict_blocks = []\n",
    "    for i in range(0, min(len(shuffled_dict), dict_blocks_needed * words_per_block), words_per_block):\n",
    "        block = \"\\n\".join(shuffled_dict[i:i + words_per_block])\n",
    "        dict_blocks.append(block)\n",
    "    \n",
    "    print(f\"Created {len(dict_blocks):,} dictionary blocks\")\n",
    "    \n",
    "    # Interleave: verse, dict, verse, dict, ...\n",
    "    verse_idx = 0\n",
    "    dict_idx = 0\n",
    "    \n",
    "    while verse_idx < len(verses):\n",
    "        # Add a verse\n",
    "        corpus_parts.append(verses[verse_idx])\n",
    "        verse_idx += 1\n",
    "        \n",
    "        # Occasionally add a dictionary block (based on ratio)\n",
    "        if random.random() < dict_ratio and dict_idx < len(dict_blocks):\n",
    "            corpus_parts.append(dict_blocks[dict_idx])\n",
    "            dict_idx += 1\n",
    "    \n",
    "    return \"\\n\\n\".join(corpus_parts)\n",
    "\n",
    "corpus = build_dual_corpus(sixteen_bar_verses, vocab_words, dict_ratio=0.25)\n",
    "print(f\"\\nTotal corpus length: {len(corpus):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character vocabulary size: 261\n",
      "Characters: \n",
      " !\"#$%&'()*+,-./:;<?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\\^... (showing first 50)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2e. Build Character Vocabulary\n",
    "# -----------------------------------------\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Character vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:50])}... (showing first 50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tensor shape: torch.Size([1614351])\n",
      "Train: 1,452,915 chars | Val: 161,436 chars\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2f. Encode Corpus to Tensor\n",
    "# -----------------------------------------\n",
    "encoded = torch.tensor([stoi[c] for c in corpus], dtype=torch.long)\n",
    "print(f\"Encoded tensor shape: {encoded.shape}\")\n",
    "\n",
    "# Train/val split (90/10)\n",
    "split_idx = int(len(encoded) * 0.9)\n",
    "train_data = encoded[:split_idx]\n",
    "val_data = encoded[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_data):,} chars | Val: {len(val_data):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: x=torch.Size([64, 128]), y=torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Configurable batch generator\n",
    "# -----------------------------------------\n",
    "BLOCK_SIZE = 128  # Context length (chars the model sees at once)\n",
    "\n",
    "def get_batch(split='train', batch_size=64):\n",
    "    \"\"\"Generate a batch of training examples.\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Random starting indices\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE - 1, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i:i + BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + BLOCK_SIZE + 1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Test batch\n",
    "x_test, y_test = get_batch()\n",
    "print(f\"Batch shapes: x={x_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Architecture ‚Äî LSTM-Based RhymeLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RhymeLM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based character language model.\n",
    "    \n",
    "    Architecture:\n",
    "    - Character embedding layer\n",
    "    - Multi-layer LSTM (captures sequential dependencies)\n",
    "    - Dropout for regularization\n",
    "    - Linear projection to vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding: characters ‚Üí vectors\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # LSTM backbone\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier initialization for better training dynamics.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len) token indices\n",
    "            hidden: optional (h_0, c_0) tuple for LSTM\n",
    "            \n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size)\n",
    "            hidden: updated (h_n, c_n) tuple\n",
    "        \"\"\"\n",
    "        # Embed characters\n",
    "        emb = self.embed(x)  # (B, T, E)\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        # LSTM\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(emb)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(emb, hidden)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        logits = self.fc(lstm_out)  # (B, T, V)\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state with zeros.\"\"\"\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 3,878,917\n",
      "RhymeLM(\n",
      "  (embed): Embedding(261, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=261, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Instantiate Model\n",
    "# -----------------------------------------\n",
    "model = RhymeLM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Training Configuration\n",
    "# -----------------------------------------\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50000\n",
    "EVAL_INTERVAL = 1000\n",
    "SAMPLE_INTERVAL = 5000\n",
    "CHECKPOINT_INTERVAL = 10000\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Loss Estimation Function\n",
    "# -----------------------------------------\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters=100):\n",
    "    \"\"\"Estimate loss on train and val sets.\"\"\"\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        batch_losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split, BATCH_SIZE)\n",
    "            logits, _ = model(x)\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, V), y.view(B * T))\n",
    "            batch_losses.append(loss.item())\n",
    "        losses[split] = np.mean(batch_losses)\n",
    "    \n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Generation Function (for sampling during training)\n",
    "# -----------------------------------------\n",
    "@torch.no_grad()\n",
    "def generate_verse(start_text=\" \", num_bars=16, temperature=0.8, max_chars=2000):\n",
    "    \"\"\"\n",
    "    Generate a verse with the specified number of bars (lines).\n",
    "    \n",
    "    Args:\n",
    "        start_text: Initial characters to seed generation\n",
    "        num_bars: Number of lines to generate\n",
    "        temperature: Sampling temperature (lower = more conservative)\n",
    "        max_chars: Safety limit on generation length\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode starting text\n",
    "    tokens = [stoi.get(c, 0) for c in start_text]\n",
    "    x = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(1)\n",
    "    generated = list(start_text)\n",
    "    bar_count = 0\n",
    "    \n",
    "    while bar_count < num_bars and len(generated) < max_chars:\n",
    "        logits, hidden = model(x, hidden)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        next_char = itos[next_token.item()]\n",
    "        generated.append(next_char)\n",
    "        \n",
    "        if next_char == '\\n':\n",
    "            bar_count += 1\n",
    "        \n",
    "        x = next_token\n",
    "    \n",
    "    model.train()\n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Checkpoint Functions\n",
    "# -----------------------------------------\n",
    "def save_checkpoint(path=\"rhyme_lm_v2.pt\", step=0):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scheduler_state': scheduler.state_dict(),\n",
    "        'stoi': stoi,\n",
    "        'itos': itos,\n",
    "        'vocab_size': vocab_size,\n",
    "        'block_size': BLOCK_SIZE,\n",
    "        'config': {\n",
    "            'embed_dim': 256,\n",
    "            'hidden_dim': 512,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.2\n",
    "        }\n",
    "    }, path)\n",
    "    print(f\"üíæ Saved checkpoint to {path}\")\n",
    "\n",
    "def load_checkpoint(path=\"rhyme_lm_v2.pt\"):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "    print(f\"‚úÖ Loaded checkpoint from {path} (step {checkpoint['step']})\")\n",
    "    return checkpoint['step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Config: 50000 steps, batch_size=64, lr=0.001\n",
      "============================================================\n",
      "Step   1000 | train loss: 1.6452 | val loss: 1.7123 | lr: 0.000999\n",
      "Step   2000 | train loss: 1.4305 | val loss: 1.6247 | lr: 0.000996\n",
      "Step   3000 | train loss: 1.3171 | val loss: 1.6045 | lr: 0.000991\n",
      "Step   4000 | train loss: 1.2179 | val loss: 1.6133 | lr: 0.000984\n",
      "Step   5000 | train loss: 1.1498 | val loss: 1.6213 | lr: 0.000976\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " expedent\n",
      "Watch the new York state of mind\n",
      "We can see the roof comes off, 'til the roof comes off, 'til the truth I did\n",
      "And I'm a friend and the place\n",
      "They say the police in the death of the streets\n",
      "When they see a lot of guys, we lookin' for her\n",
      "The new state of the real motherfuckers as white tits, it's like Kobe marrie\n",
      "My daddy crack me at the court, the millions are real\n",
      "\n",
      "========================================\n",
      "\n",
      "Step   6000 | train loss: 1.1025 | val loss: 1.6326 | lr: 0.000965\n",
      "Step   7000 | train loss: 1.0540 | val loss: 1.6485 | lr: 0.000952\n",
      "Step   8000 | train loss: 1.0167 | val loss: 1.6681 | lr: 0.000938\n",
      "Step   9000 | train loss: 0.9832 | val loss: 1.6864 | lr: 0.000922\n",
      "Step  10000 | train loss: 0.9596 | val loss: 1.6981 | lr: 0.000905\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " the call a brown nigga and make 'em locked up\n",
      "They can be the one of the ends\n",
      "Only time man are gonna kill me\n",
      "But I was active by the ball straight\n",
      "But I'm all about the family love on my sans\n",
      "I look like a lanchine, I'm tryna be home strikes\n",
      "I don't trust it, I ain't even when you need that shit out of love\n",
      "You just gotta keep me hoped up with the shit weed\n",
      "\n",
      "========================================\n",
      "\n",
      "üíæ Saved checkpoint to rhyme_lm_v2_step10000.pt\n",
      "Step  11000 | train loss: 0.9281 | val loss: 1.7049 | lr: 0.000885\n",
      "Step  12000 | train loss: 0.9070 | val loss: 1.7211 | lr: 0.000864\n",
      "Step  13000 | train loss: 0.8921 | val loss: 1.7309 | lr: 0.000842\n",
      "Step  14000 | train loss: 0.8767 | val loss: 1.7387 | lr: 0.000819\n",
      "Step  15000 | train loss: 0.8613 | val loss: 1.7434 | lr: 0.000794\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " love you, I don't even know if that I'm seeing it (Ooh)\n",
      "You like the break of dawn and he don't wanna go to slide, ayy\n",
      "Can't let this one slide, ayy\n",
      "Don't you wanna dance with me? No?\n",
      "I could never lit on your nigga one\n",
      "If you weren't completed your body rocks at the laws to let me die\n",
      "To be the past of love at your money\n",
      "Get the streets, you better lose yourself in the music\n",
      "\n",
      "========================================\n",
      "\n",
      "Step  16000 | train loss: 0.8403 | val loss: 1.7522 | lr: 0.000768\n",
      "Step  17000 | train loss: 0.8270 | val loss: 1.7563 | lr: 0.000741\n",
      "Step  18000 | train loss: 0.8058 | val loss: 1.7730 | lr: 0.000713\n",
      "Step  19000 | train loss: 0.7997 | val loss: 1.7722 | lr: 0.000684\n",
      "Step  20000 | train loss: 0.7831 | val loss: 1.7907 | lr: 0.000655\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " liquor and my phone line\n",
      "I been doin' buckin' from the gangs again\n",
      "I got a ting from the joke\n",
      "And then that verse like I can't bring the room\n",
      "I see the real good shit, I wear my voice\n",
      "If I have no pour with the pictures of my haters\n",
      "That make it real move, yeah, you gotta do that sentence\n",
      "I promise to be the fame (Yeah)\n",
      "\n",
      "========================================\n",
      "\n",
      "üíæ Saved checkpoint to rhyme_lm_v2_step20000.pt\n",
      "Step  21000 | train loss: 0.7797 | val loss: 1.7941 | lr: 0.000624\n",
      "Step  22000 | train loss: 0.7654 | val loss: 1.7945 | lr: 0.000594\n",
      "Step  23000 | train loss: 0.7504 | val loss: 1.8205 | lr: 0.000563\n",
      "Step  24000 | train loss: 0.7421 | val loss: 1.8319 | lr: 0.000531\n",
      "Step  25000 | train loss: 0.7370 | val loss: 1.8259 | lr: 0.000500\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " Love County Thous ()\n",
      "Chris Brown ‚Äî Burn My Gnest ()\n",
      "Quavo ‚Äî The Motion Rolling Songs ()\n",
      "February\n",
      "Billie Eilish ‚Äî Superorganism ()\n",
      "Charlotte Lawrence ‚Äî \"Your Soul\" ()\n",
      "DJ Khaled (feat. A$AP Rocky) ‚Äî \"Bone Thinking\" ()\n",
      "The Christmas ‚Äî Stay Treat ()\n",
      "\n",
      "========================================\n",
      "\n",
      "Step  26000 | train loss: 0.7180 | val loss: 1.8390 | lr: 0.000469\n",
      "Step  27000 | train loss: 0.7127 | val loss: 1.8495 | lr: 0.000437\n",
      "Step  28000 | train loss: 0.7022 | val loss: 1.8426 | lr: 0.000406\n",
      "Step  29000 | train loss: 0.6964 | val loss: 1.8496 | lr: 0.000376\n",
      "Step  30000 | train loss: 0.6808 | val loss: 1.8659 | lr: 0.000345\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " the nonsense, release her niggas call for me\n",
      "It go straight for the whole world point no more\n",
      "And address me, I put the game from a spot\n",
      "I'm startin' to think all the same slime that I think I'm goin' to Two Ditonity\n",
      "New Yorkin' for the life of a drug life\n",
      "I could be the way I'm diving in your top\n",
      "What is a right be a whip, I wanna get it on till I die?\n",
      "Get it on till I die, get it on till I die\n",
      "\n",
      "========================================\n",
      "\n",
      "üíæ Saved checkpoint to rhyme_lm_v2_step30000.pt\n",
      "Step  31000 | train loss: 0.6786 | val loss: 1.8599 | lr: 0.000316\n",
      "Step  32000 | train loss: 0.6662 | val loss: 1.8692 | lr: 0.000287\n",
      "Step  33000 | train loss: 0.6584 | val loss: 1.8810 | lr: 0.000259\n",
      "Step  34000 | train loss: 0.6532 | val loss: 1.8871 | lr: 0.000232\n",
      "Step  35000 | train loss: 0.6551 | val loss: 1.8952 | lr: 0.000206\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " daddy and mama (Nuh)\n",
      "No rollin' out of this confession\n",
      "It's on 'til the broken hearts to somebody too much in the states\n",
      "I sent her to my back end dancents\n",
      "Thug store, yeah, woah\n",
      "\n",
      "And I know I give a fuck about us\n",
      "But when I start to rise\n",
      "\n",
      "========================================\n",
      "\n",
      "Step  36000 | train loss: 0.6483 | val loss: 1.8941 | lr: 0.000181\n",
      "Step  37000 | train loss: 0.6307 | val loss: 1.8911 | lr: 0.000158\n",
      "Step  38000 | train loss: 0.6333 | val loss: 1.8867 | lr: 0.000136\n",
      "Step  39000 | train loss: 0.6340 | val loss: 1.8999 | lr: 0.000115\n",
      "Step  40000 | train loss: 0.6261 | val loss: 1.9101 | lr: 0.000095\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " to be your Superman\n",
      "Can't be your Superman, can't be your Superman\n",
      "Can't be your Superman, can't be your Superman\n",
      "I can't be your Superman, can't be your Superman\n",
      "I can't be your Superman, can't be your Superman\n",
      "I can't be your Superman, can't be your Superman\n",
      "Can't be your Superman, can't be your Superman\n",
      "I can't be your Superman, can't be your Superman\n",
      "\n",
      "========================================\n",
      "\n",
      "üíæ Saved checkpoint to rhyme_lm_v2_step40000.pt\n",
      "Step  41000 | train loss: 0.6245 | val loss: 1.9159 | lr: 0.000078\n",
      "Step  42000 | train loss: 0.6162 | val loss: 1.9021 | lr: 0.000062\n",
      "Step  43000 | train loss: 0.6117 | val loss: 1.9088 | lr: 0.000048\n",
      "Step  44000 | train loss: 0.6186 | val loss: 1.9201 | lr: 0.000035\n",
      "Step  45000 | train loss: 0.6105 | val loss: 1.9248 | lr: 0.000024\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " we can shoot the record, yeah\n",
      "Yeah, not fucked up sound (Yeah)\n",
      "The sun ride (Roll)\n",
      "You in a bright school with my niggas and shot (Our gene‚Äîour gene‚Äîour generation)\n",
      "Oh\n",
      "\n",
      "Back-rubs on backbones and black booty\n",
      "Ain't nothing else popping out here at  AM\n",
      "\n",
      "========================================\n",
      "\n",
      "Step  46000 | train loss: 0.6169 | val loss: 1.9174 | lr: 0.000016\n",
      "Step  47000 | train loss: 0.6169 | val loss: 1.9127 | lr: 0.000009\n",
      "Step  48000 | train loss: 0.6077 | val loss: 1.9102 | lr: 0.000004\n",
      "Step  49000 | train loss: 0.6019 | val loss: 1.9213 | lr: 0.000001\n",
      "Step  50000 | train loss: 0.6103 | val loss: 1.9163 | lr: 0.000000\n",
      "\n",
      "========================================\n",
      "üé§ SAMPLE VERSE:\n",
      "========================================\n",
      " that, I'm the feelin' of golden\n",
      "Me and Change got a brand new spare\n",
      "My adversaries are going crazy\n",
      "But if you profist me, baby, you know I want you too\n",
      "They call me Superman, I'm here to rescue you\n",
      "I wanna save you, girl, come be in Shady's world (Ooh-ooh)\n",
      "Oh boy, you drive me crazy, bitch, you make me hurl‚ÄòCause I can't be your Superman, can't be your Superman\n",
      "Can't be your Superman, can't be your Superman\n",
      "\n",
      "========================================\n",
      "\n",
      "üíæ Saved checkpoint to rhyme_lm_v2_step50000.pt\n",
      "\n",
      "‚úÖ Training complete!\n",
      "üíæ Saved checkpoint to rhyme_lm_v2_final.pt\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Main Training Loop\n",
    "# -----------------------------------------\n",
    "print(\"Starting training...\")\n",
    "print(f\"Config: {NUM_STEPS} steps, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for step in range(1, NUM_STEPS + 1):\n",
    "    # Get batch\n",
    "    x, y = get_batch('train', BATCH_SIZE)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, _ = model(x)\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(B * T, V), y.view(B * T))\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Logging\n",
    "    if step % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Step {step:>6} | train loss: {losses['train']:.4f} | val loss: {losses['val']:.4f} | lr: {lr:.6f}\")\n",
    "    \n",
    "    # Sample generation\n",
    "    if step % SAMPLE_INTERVAL == 0:\n",
    "        print(\"\\n\" + \"=\" * 40)\n",
    "        print(\"üé§ SAMPLE VERSE:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(generate_verse(num_bars=8, temperature=0.7))\n",
    "        print(\"=\" * 40 + \"\\n\")\n",
    "    \n",
    "    # Checkpointing\n",
    "    if step % CHECKPOINT_INTERVAL == 0:\n",
    "        save_checkpoint(f\"rhyme_lm_v2_step{step}.pt\", step)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "save_checkpoint(\"rhyme_lm_v2_final.pt\", NUM_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Generation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ ==================================================\n",
      "Yeah, oh-oh-oh\n",
      "Oh-oh, oh-oh, oh-oh\n",
      "Oh-oh-oh\n",
      "Oh-oh, oh-oh, oh-oh\n",
      "Oh-oh-oh\n",
      "Oh-oh, oh-oh, oh-oh\n",
      "Bitch, don't get too comfortable\n",
      "Better not get too comfortable\n",
      "Better not get too comfortable\n",
      "Four or five hoes, stuff 'em in a Lamborghini, yeah (Move your feet)\n",
      "We been on the low for a while\n",
      "I been seeing the setter from the hood, grrah, brother\n",
      "Ball in his way seems so much from the same thing that we were in the back\n",
      "Close to the bag on, and we just did addicting the globe\n",
      "\n",
      "Bitch, don't get too comfortable\n",
      "\n",
      "=====================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yeah, oh-oh-oh\\nOh-oh, oh-oh, oh-oh\\nOh-oh-oh\\nOh-oh, oh-oh, oh-oh\\nOh-oh-oh\\nOh-oh, oh-oh, oh-oh\\nBitch, don't get too comfortable\\nBetter not get too comfortable\\nBetter not get too comfortable\\nFour or five hoes, stuff 'em in a Lamborghini, yeah (Move your feet)\\nWe been on the low for a while\\nI been seeing the setter from the hood, grrah, brother\\nBall in his way seems so much from the same thing that we were in the back\\nClose to the bag on, and we just did addicting the globe\\n\\nBitch, don't get too comfortable\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Generate a Full 16-Bar Verse\n",
    "# -----------------------------------------\n",
    "def write_16(prompt=\"I \", temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate a 16-bar verse.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text to seed the generation\n",
    "        temperature: Controls randomness (0.5=conservative, 1.0=creative)\n",
    "    \"\"\"\n",
    "    verse = generate_verse(start_text=prompt, num_bars=16, temperature=temperature)\n",
    "    print(\"üé§ \" + \"=\" * 50)\n",
    "    print(verse)\n",
    "    print(\"=\" * 53)\n",
    "    return verse\n",
    "\n",
    "# Try it out!\n",
    "write_16(prompt=\"Yeah, \", temperature=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPERATURE COMPARISON\n",
      "============================================================\n",
      "\n",
      "üå°Ô∏è Temperature: 0.5\n",
      "----------------------------------------\n",
      "I ever say girl there's no longer than me\n",
      "I wanna see the family way back to the floor now\n",
      "I know they tryna make it but I did it (Huh?)\n",
      "I see no case and she said \"No comments\"\n",
      "\n",
      "\n",
      "\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "----------------------------------------\n",
      "I start a few outlaw like selfish\n",
      "Maybe that's your words to see your man\n",
      "You the one who did that girl in the margin\n",
      "Tryna see the corner put on some and strapped with a golden chare\n",
      "\n",
      "\n",
      "\n",
      "üå°Ô∏è Temperature: 0.9\n",
      "----------------------------------------\n",
      "I take it, I got a flo life\n",
      "Say she reople that family hustler, you're still out here\n",
      "But it's in a party's side, when we bigger white like Dallaze\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "üå°Ô∏è Temperature: 1.0\n",
      "----------------------------------------\n",
      "I didn't deal with the best when I gotta start tlied struggle, and I got to come up\n",
      "When the hood runnin' out the park study and bitches\n",
      "I moved in those S with the G case and drink, I'm Halido (And by the guy)\n",
      "On That Dre, to the C. Nicki (Wrose with me, P\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Temperature Comparison\n",
    "# -----------------------------------------\n",
    "print(\"TEMPERATURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.5, 0.7, 0.9, 1.0]:\n",
    "    print(f\"\\nüå°Ô∏è Temperature: {temp}\")\n",
    "    print(\"-\" * 40)\n",
    "    verse = generate_verse(start_text=\"I \", num_bars=4, temperature=temp)\n",
    "    print(verse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if train_losses and val_losses:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    steps = [i * EVAL_INTERVAL for i in range(1, len(train_losses) + 1)]\n",
    "    \n",
    "    plt.plot(steps, train_losses, label='Train Loss', alpha=0.8)\n",
    "    plt.plot(steps, val_losses, label='Val Loss', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('RhymeLM v2 Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curve.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history to plot yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to continue training from checkpoint:\n",
    "# start_step = load_checkpoint(\"rhyme_lm_v2_step10000.pt\")\n",
    "\n",
    "def train_more(additional_steps=10000, batch_size=64):\n",
    "    \"\"\"Continue training for more steps.\"\"\"\n",
    "    print(f\"Training for {additional_steps:,} more steps...\")\n",
    "    \n",
    "    for step in range(1, additional_steps + 1):\n",
    "        x, y = get_batch('train', batch_size)\n",
    "        \n",
    "        logits, _ = model(x)\n",
    "        B, T, V = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B * T, V), y.view(B * T))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step}/{additional_steps}, loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            print(\"\\n--- Sample ---\")\n",
    "            print(generate_verse(num_bars=4, temperature=0.7))\n",
    "            print(\"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "# train_more(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "‚úÖ You're good to go with NVIDIA!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"‚úÖ You're good to go with NVIDIA!\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"‚úÖ Apple Silicon (MPS) available\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected - running on CPU\")\n",
    "    print(\"\\nFor NVIDIA on Fedora, try:\")\n",
    "    print(\"  sudo dnf install akmod-nvidia xorg-x11-drv-nvidia-cuda\")\n",
    "    print(\"  pip install torch --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
